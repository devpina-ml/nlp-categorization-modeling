# -*- coding: utf-8 -*-
"""PINA_coba_df_new.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1BYDhV47IOyp_k2a8B1YOwaGBGBvemNKt
"""

! pip install transformers==4.28.0
! pip install accelerate
! pip install datasets
! pip install rouge-score nltk
! pip install huggingface_hub
! pip install sentencepiece
! pip install evaluate
! pip install gdown
! pip install git-lfs

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import re
import string

import nltk
nltk.download('stopwords')

import warnings
warnings.filterwarnings('ignore')

df = pd.read_csv('/content/df_new_fix.csv', header=0)

df

df.info()

df.isnull().sum()

df.drop_duplicates(subset=['description'], inplace=True)

df.shape

plt.figure(figsize=(15,10))
sns.countplot(data=df, x='category')
plt.xticks(rotation=90)
plt.show()

from collections import Counter

Counter(df['category'])

from sklearn.preprocessing import LabelEncoder

le = LabelEncoder()
df['category'] = le.fit_transform(df['category'])

keys = le.classes_
values = le.transform(le.classes_)
dictionary = dict(zip(keys, values))
print(dictionary)

re = {v:k for k,v in dictionary.items()}
print(re)

"""# Modeling"""

df = df.reset_index(drop=True)

from huggingface_hub import notebook_login

notebook_login()

from sklearn.model_selection import train_test_split

df_trans = df.copy()

train, valid = train_test_split(df_trans, test_size=0.2, random_state=42, stratify=df_trans.category)

train.reset_index(drop=True, inplace=True)

valid.reset_index(drop=True, inplace=True)

train = train.rename(columns={'description': 'text', 'category': 'label'})

valid = valid.rename(columns={'description': 'text', 'category': 'label'})

from datasets import Dataset

train_dataset = Dataset.from_dict(train)
valid_dataset = Dataset.from_dict(valid)

import datasets
dd = datasets.DatasetDict({"train":train_dataset,"validation":valid_dataset})

from transformers import AutoTokenizer

tokenizer = AutoTokenizer.from_pretrained("distilbert-base-uncased")

def tokenize_function(examples):
    return tokenizer(examples["text"], truncation=True, padding='max_length')

tokenized_datasets = dd.map(tokenize_function, batched=True)

print(tokenized_datasets['train'][0])

tokenized_datasets

tokenized_datasets = tokenized_datasets.remove_columns(
    'text'
)

from transformers import DataCollatorWithPadding

data_collator = DataCollatorWithPadding(tokenizer=tokenizer)

data_collator(tokenized_datasets["train"][1])

import evaluate

#accuracy = evaluate.load("accuracy")

import numpy as np

def compute_metrics(eval_pred):
    metric1 = evaluate.load("accuracy")
    metric2 = evaluate.load("precision")
    metric3 = evaluate.load("recall")
    
    logits, labels = eval_pred
    predictions = np.argmax(logits, axis=-1)
    accuracy = metric1.compute(predictions=predictions, references=labels)['accuracy']
    precision = metric2.compute(predictions=predictions, references=labels, average='macro')["precision"]
    recall = metric3.compute(predictions=predictions, references=labels, average='macro')["recall"]
    return {"accuracy": accuracy, "precision": precision, "recall": recall}

id2label = {0: 'Beauty & Care', 1: 'Books & Stationery', 2: 'Business Expense', 3: 'Car Loan', 4: 'Cash Withdrawal', 5: 'Credit Card', 6: 'Dentist', 7: 'Donation & Social', 8: 'Education', 9: 'Electricity & Water Bills', 10: 'Entertainment', 11: 'Eye Care', 12: 'Family', 13: 'Fashion', 14: 'Fees & Charges', 15: 'Food & Drink', 16: 'Gadget & Electronics', 17: 'Gas & Fuel', 18: 'Gifts', 19: 'Groceries & General', 20: 'Health & Medical', 21: 'Home / Rent', 22: 'Home Loan', 23: 'Household Tools', 24: 'Insurance', 25: 'Investment', 26: 'Kids', 27: 'Laundry', 28: 'Loan', 29: 'Pets', 30: 'Printshop', 31: 'Public Transport', 32: 'Service & Auto Parts', 33: 'Services Needs', 34: 'Shipping', 35: 'Shopping', 36: 'Sports', 37: 'Subscription', 38: 'Taxes', 39: 'Telephone & Internet', 40: 'Top Up', 41: 'Transport', 42: 'Travel'}
label2id = {'Beauty & Care': 0, 'Books & Stationery': 1, 'Business Expense': 2, 'Car Loan': 3, 'Cash Withdrawal': 4, 'Credit Card': 5, 'Dentist': 6, 'Donation & Social': 7, 'Education': 8, 'Electricity & Water Bills': 9, 'Entertainment': 10, 'Eye Care': 11, 'Family': 12, 'Fashion': 13, 'Fees & Charges': 14, 'Food & Drink': 15, 'Gadget & Electronics': 16, 'Gas & Fuel': 17, 'Gifts': 18, 'Groceries & General': 19, 'Health & Medical': 20, 'Home / Rent': 21, 'Home Loan': 22, 'Household Tools': 23, 'Insurance': 24, 'Investment': 25, 'Kids': 26, 'Laundry': 27, 'Loan': 28, 'Pets': 29, 'Printshop': 30, 'Public Transport': 31, 'Service & Auto Parts': 32, 'Services Needs': 33, 'Shipping': 34, 'Shopping': 35, 'Sports': 36, 'Subscription': 37, 'Taxes': 38, 'Telephone & Internet': 39, 'Top Up': 40, 'Transport': 41, 'Travel': 42}

"""# Kalau mau hypetuning ini gak perlu dirun"""

from transformers import AutoModelForSequenceClassification, TrainingArguments, Trainer

model = AutoModelForSequenceClassification.from_pretrained(
    "distilbert-base-uncased", num_labels=43, id2label=id2label, label2id=label2id
)

batch_size = 16
num_train_epochs = 10
# Show the training loss with every epoch
logging_steps = len(tokenized_datasets["train"]) // batch_size

training_args = TrainingArguments(
    output_dir='distilbert-base-uncased-PINA-dfnew-tuning',
    learning_rate=2e-4,
    per_device_train_batch_size=batch_size,
    per_device_eval_batch_size=batch_size,
    num_train_epochs=num_train_epochs,
    weight_decay=0.0001,
    evaluation_strategy="epoch",
    save_strategy="epoch",
    load_best_model_at_end=True,
    push_to_hub=True,
    logging_steps=logging_steps
)

trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=tokenized_datasets["train"],
    eval_dataset=tokenized_datasets["validation"],
    tokenizer=tokenizer,
    data_collator=data_collator,
    compute_metrics=compute_metrics,
)

trainer.train()

trainer.evaluate()

trainer.push_to_hub()

"""#Hyperparameter Tuning"""

from transformers import AutoModelForSequenceClassification, TrainingArguments, Trainer, AutoConfig

! pip install ray[tune]

#train_dataset = encoded_dataset["train"].shard(index=1, num_shards=10) #Buat bikin partisi, jadi gak seluruh datanya bakal ditrain (makan waktu)

db_config_base = AutoConfig.from_pretrained("distilbert-base-uncased")

db_config_base

from ray import tune

def model_init(params):
        db_config = db_config_base
        if params is not None:
            db_config.update({'dropout': params['dropout'],'num_labels':43, 'id2label':id2label, 'label2id':label2id})
        return AutoModelForSequenceClassification.from_pretrained("distilbert-base-uncased", config=db_config)

def hp_space_ray(trial):
    return {
        "learning_rate": tune.choice([2e-5, 2e-4, 2e-3]),
        "per_device_train_batch_size": tune.choice([8, 16, 24, 32]),
        "per_device_eval_batch_size":tune.choice([8, 16, 24, 32]),
        "weight_decay": tune.choice([1e-4, 1e-3, 1e-2]),
        "dropout" : tune.choice([0.1, 0.2, 0.3, 0.4])
    }

#!pip install --upgrade accelerate

training_args = TrainingArguments(
    output_dir='distilbert-base-uncased-PINA-hypetune',
    evaluation_strategy='epoch',
    eval_steps=500,
    gradient_accumulation_steps=1000,
    eval_accumulation_steps=1
)

trainer = Trainer(
    model_init=model_init,
    args=training_args,
    train_dataset=tokenized_datasets["train"],
    eval_dataset=tokenized_datasets["validation"],
    tokenizer=tokenizer,
    data_collator=data_collator,
    compute_metrics=compute_metrics,
)

best_trial = trainer.hyperparameter_search(
    hp_space=hp_space_ray,
    direction="maximize", 
    backend="ray",
    n_trials=7)

print(best_trial.hyperparameters.items())

trainer.args

for n, v in best_run.hyperparameters.items():
  print(n, v)
    #setattr(trainer.args, n, v)

#trainer.train()

type(best_trial.hyperparameters.items())

import json

# create an example dictionary
best_parameter = dict(best_trial.hyperparameters.items())

# write the dictionary to a JSON file
with open('raytune_pina.json', 'w') as f:
    json.dump(best_parameter, f)